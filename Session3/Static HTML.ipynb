{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtaining, parsing and structuring static HTML websites\n",
    "\n",
    "In this notebook we will learn how to scrape basic static, i.e. non-interactive HTML-based websites. We will\n",
    "- learn best practices of Webscraping\n",
    "- learn to efficiently search text objects using `RegEx`\n",
    "- obtain the HTML raw content using the `requests` module\n",
    "- convert the raw HTML into a format that is easier to search, or parse, using the `BeautifulSoup` module\n",
    "- learn how to identify the elements of interest in the raw HTML using the browser's inspect functionality and the CSS SelectorGadget\n",
    "- construct a table, or dataframe, with the popular table calculation module `pandas` and store the output locally in a standard spreadsheet format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rules of the game\n",
    "(*Recommendations taken from Pablo Barberá's course on Webscraping*)\n",
    "\n",
    "1. Respect the hosting site's wishes:\n",
    "\n",
    "    - Check if an API exists or if data are available for download\n",
    "    - Keep in mind where data comes from and give credit (and respect copyright if you wnat to republish the data!)\n",
    "    - Some websites **disallow** scrapers on `robots.txt` file\n",
    "    \n",
    "2. Limit your bandwidth use:\n",
    "    \n",
    "    - Wait one or two seconds after each hit\n",
    "    - Scrape only what you need, and just once (e.g. store the html file in disk, then parse it)\n",
    "\n",
    "3. When using APIs, read documentation\n",
    "\n",
    "    - Is there a batch download option?\n",
    "    - Are there any rate limits?\n",
    "    - Can you share the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The art of webscraping\n",
    "\n",
    "Workflow:\n",
    "\n",
    "1. Learn about structure of website\n",
    "2. Choose your strategy (static vs. dynamic, identification of elements, scale & backend)\n",
    "3. Build prototype code: extract, prepare, validate\n",
    "4. Generalize: functions, loops, debugging\n",
    "5. Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTML Basics: A primer\n",
    "\n",
    "- HTML (= Hypertext Markup Language): a raw \"text\" file interpreted by an internet browser\n",
    "- structure defined by `tags`\n",
    "    1. opening and closing `<>` `</>`\n",
    "    2. **attributes** of the element inside of tag\n",
    "    3. The **text** to be structured\n",
    "\n",
    "<img src=\"https://www.w3schools.com/js/pic_htmltree.gif\" alt=\"Drawing\" style=\"width: 500px;\">\n",
    "\n",
    "\n",
    "Example:\n",
    "\n",
    "```html\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<body>\n",
    "    \n",
    "    <h1>My First Heading</h1>\n",
    "    <p>My first paragraph.</p>\n",
    "    <a href=\"https://hu-berlin.de/\">Link to HU Berlin</a>      # text to appear comes after link\n",
    "    \n",
    "</body>\n",
    "    \n",
    "</html>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# browser's interpretation\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML('<!DOCTYPE html> <html> <body> <h1>My First Heading</h1> <p>My first paragraph. <br> <body> <strong> Lorem ipsum... </strong> <br> Another text </body></p> <a href=\"https://hu-berlin.de/\">Link to HU Berlin</a> </body> </html>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some common tags:\n",
    "\n",
    "- Document elements: `<head>`, `<body>`, `<footer>`...\n",
    "- Document components: `<title>`, `<h1>`, `<div>`...\n",
    "- Text style: `<b>`, `<i>`, `<strong>`...\n",
    "- Hyperlinks: `<a>`\n",
    "\n",
    "## Some additional website components:\n",
    "\n",
    "**CSS**\n",
    "\n",
    "- Cascading Style Sheets (CSS) describe the formatting and e.g. colors of HTML components (e.g. `<h1>`, `<div>`,...)\n",
    "- CSS is useful because we can use CSS pointers (selectors) to identify our HTML elements of interest\n",
    "\n",
    "**Javascript**\n",
    "\n",
    "- Javascript extends the functionality of websites (e.g. a change of content after loading of a website)\n",
    "- Javascript is executed on the client-side and not server-side!\n",
    "- Therefore, JS based websites are a big problem for us as client-side changes of the HTML document cannot be captured using conventional `requests`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTML inspection in a browser\n",
    "\n",
    "- Open the page https://www.ecb.europa.eu/ and open the Developer Tools with key `F12` (alternatively right click + Inspect)\n",
    "- Hover over the different elements and observe how the elements are being highlighted\n",
    "- In the Developer section you can access all information regarding specific elements, e.g. `id`, `class` etc. as well as traffic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: ResearchGate.net - is scraping allowed?\n",
    "\n",
    "[ResearchGate.net](https://de.wikipedia.org/wiki/ResearchGate) is a social network for researchers.\n",
    "\n",
    "First check: `robots.txt` file: https://www.researchgate.net/robots.txt\n",
    "\n",
    "````\n",
    "User-agent: *\n",
    "Allow: /\n",
    "Disallow: /connector/\n",
    "Disallow: /plugins.\n",
    "Disallow: /firststeps.\n",
    "Disallow: /publicliterature.PublicLiterature.search.html\n",
    "Disallow: /lite.publication.PublicationRequestFulltextPromo.requestFulltext.html\n",
    "Disallow: /amp/authorize\n",
    "Allow: /signup.SignUp.html\n",
    "Disallow: /signup.\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- User-agent: * means that the following sections apply for **any** user-agent (e.g. Google Bots or our Python programs).\n",
    "- This file defines which sections (`domains`) are prohibited to be scraped, e.g. `/connector/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As such, it would be permissible to scrape e.g. the job ads under the domain (`/jobs/`):\n",
    "\n",
    "![alt text](researchgate_jobs.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**But** in the [Terms of Service by researchgate](https://www.researchgate.net/application.TermsAndConditions.html) it is clearly stated, that the operators of this website do not allow wescraping:\n",
    "\n",
    "![alt text](researchgate_tac.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "- You should not scrape the website in any case without the explicit permission by the website operators\n",
    "- (and you won't get it - other colleagues have tried several times :P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about https://www.ecb.europa.eu/?\n",
    "- Check the robots.txt and define a `list` containing all prohibited domains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BeautifulSoup\n",
    "\n",
    "BeautifulSoup is a Python Parsing Package which \"understands\" HTML and XML Strings and can represent them in an aforementioned tree structure.\n",
    "```python\n",
    "!conda install pip\n",
    "!pip install beautifulsoup4\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html_doc = \"\"\"<html><head><title>The Dormouse's story</title></head>\n",
    "<body>\n",
    "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
    "\n",
    "<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
    "<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>,\n",
    "<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n",
    "<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\n",
    "and they lived at the bottom of a well.</p>\n",
    "\n",
    "<p class=\"story\">...</p>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(html_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A string containing HTML content is processed through the function `BeautifulSoup()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html_doc, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterwards, attributes can be retrieved from the tree structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.title.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the difference? How can you check?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's especially useful to efficiently search for specific tags inside the entire HTML document, e.g. `a` for links:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, elements can be identified through attributes such as `id`, `href` or `class`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('id', soup.find(id='link2'))\n",
    "print('href', soup.find(href='htpp://example.com/lacie'))\n",
    "print('class', soup.find(class_='story'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intermezzo: Regular Expressions\n",
    "\n",
    "> Some people, when confronted with a problem, think \"I know, I'll use regular expressions.\" Now they have two problems\n",
    "\n",
    "    Jamie Zawinski\n",
    "    \n",
    "Regular expressions specify subsets from a finite set of characters/symbols $\\Sigma$. They can perform three operations:\n",
    "If $x$ and $y$ are regular expressions, then\n",
    "\n",
    "1. Concatenation: ($xy$)\n",
    "2. Alternative: ($x | y$)\n",
    "3. Repetition (Kleene-star): ($x^{*}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `re`\n",
    "\n",
    "Python's module for regular expressions. Can be called as method or as function.\n",
    "\n",
    "Additional information: [PyDocs RegEx HowTo](https://docs.python.org/3/howto/regex.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "pattern = 'a'\n",
    "string = 'Spam, Eggs and Bacon'\n",
    "\n",
    "\n",
    "\n",
    "# returns only one match-object!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = 'xyz'\n",
    "# returns an empty match which cannot be grouped, i.e. returns the matched string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus: what type of object is match.group(0,1)? Is it immutable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile raw string into a proper `re` object with methods for various operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `re.findall`\n",
    "\n",
    "Finds all occurrences of a given regular expression in a string and returns it as a *list-of-strings*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(string)\n",
    "print(pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special characters\n",
    "\n",
    "1. `.` (dot) is the most general regular expression. It specifies an arbitrary character within the string.\n",
    "2. `^` (carret) refers to the beginning of the string.\n",
    "3. `$` (dollar) refers to the position in front of a newline (`\\n`) or the end of the string in `MULTILINE` mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns regex + first arbitrary character after regex\n",
    "# does the same and returns a list of strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenation\n",
    "\n",
    "Specifies strings in a certain order. The order can be reversed by adding a set `[]`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"normal\" order\n",
    "# identifies the first character's occurrence of the pattern in reversed order\n",
    "print(string)\n",
    "# specifies the set of patterns, not only the entire sequence, in an abritrary sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternatives\n",
    "\n",
    "Finds regular expression $x$ **or** $y$ and returns list. Operator is `|`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scheme: pattern, string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special characters: Summary\n",
    "\n",
    "The following characters have special meanings in regular expressions:\n",
    "\n",
    "Character | Meaning\n",
    "- | - \n",
    "`.` | Arbitrary character. With `DOTALL` including Newline (`\\n`)\n",
    "`^` | Beginning of a string. If `MULTILINE` also after each `\\n`\n",
    "`$` | End of a string. If `MULTILINE` also in front of each `\\n`\n",
    "`\\` | Escape for special characters or describe a set\n",
    "`[]` | Defines a set of characters\n",
    "`()` | Defines the scope, i.e. sets groups.\n",
    "\n",
    "if we're searching '.' (dot) then `\\.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repetitions\n",
    "\n",
    "Specifies the number of repetitions of a preceding regular expression $x$. The following repetitions are possible:\n",
    "\n",
    "Syntax | Meaning\n",
    "- | - \n",
    "`*` | 0 or more repetitions\n",
    "`+` | 1 or more repetitions\n",
    "`{m}` | Exactly `m` repetitions\n",
    "`{m,n}` | From `m` up until (including) `n`\n",
    "\n",
    "By default, repetitions are *greedy*, i.e. it is as much consumed of a string as possible. This behavior can be disabled by setting a `?` after the repetition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_string = '/2021/abcdefg'\n",
    "\n",
    "# objective: search for anything within first occurrence of / and subsequent occurrence of /\n",
    "# specify this to be from set of type digits\n",
    "# restrict matches to m=4 repetitions\n",
    "# return every match as a list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spamskit = '''The screen is filled by the face of PETER PARKER, a seventeen year\n",
    "old boy. High school must not be any fun for Peter, he's one\n",
    "hundred percent nerd- skinny, zitty, glasses. His face is just\n",
    "frozen there, a cringing expression on it, which strikes us odd\n",
    "until we realize the image is freeze framed.'''\n",
    "\n",
    "# return the regex 'ee' ocurring at least once as a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A useful regex combination, `.*?` can be used to allow for multiple placeholders (`.`) arbitrary repeatedly (`*`) **up until** the next specified pattern is initially found (`?`) - remember: *(non-)greedy*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 'eeeAiiZuuuuAoooZeeee'\n",
    "\n",
    "# 'A' followed by an arbitrary character repeated for an arbitrary number of times until closed by the last occurrence of 'Z' - greedy!\n",
    "\n",
    "# non-greedy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions of RegEx\n",
    "\n",
    "Regular expressions have a variety of syntactical nuances to simplify the specification of any possibly imaginable string.\n",
    "\n",
    "## Specification of sets\n",
    "\n",
    "Syntax (short cut) | Equivalent | Meaning\n",
    "-|-|-\n",
    "`\\d` | `[0-9]` | Integers\n",
    "`\\D` | `[^0-9]` | Anything expect integers \n",
    "`\\s` | `[ \\t\\n\\r\\f\\v]` | Anything that is whitespace\n",
    "`\\S` | `[^ \\t\\n\\r\\f\\v] ` | Anything that is not whitespace\n",
    "`\\w` | `[a-zA-Z0-9_]` | Alphanumeric characters and underscore\n",
    "`\\W` | `[^a-zA-Z0-9_]` | Anything but alphanumeric characters and underscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spamskit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Substitution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Tokenization using `re`\n",
    "\n",
    "Write a regular expression which returns a list of strings from the input string `spamskit` that contains a \"sequence\" of at least one alphanumeric character (i.e. no whitespace!) of arbitrary length (until the next non-matching character is found, i.e. the subsequent non-alphanumeric character = whitespace)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --> Back to `BeautifulSoup`\n",
    "\n",
    "Any query can be supplied with a regular expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search in attribute href"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search in attribute id: 'link' + set of integers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First attempt: ECB homepage\n",
    "\n",
    "1. Open the Anaconda Prompt and install the module `requests`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_external = 'https://www.ecb.europa.eu/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What data type is the object `seed_external`? How can you check?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxies =  {'https' : 'https://ap-python-proxy:x2o7rCPYuN1JuV8H@app-gw-2.ecb.de:8080',\n",
    "           'http'  : 'http://ap-python-proxy:x2o7rCPYuN1JuV8H@app-gw-2.ecb.de:8080'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Is this domain an admissible path? Hint: Check the `robots.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Was the request successful? How can you check the status? Hint: Check the available methods by using Jupyter's auto-complete functionality, i.e. type a dot at the end of the object you're investigating followed by <kbd>Tab</kbd>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Which method could be most informative w.r.t. actual content? How many characters long is the raw HTML file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Display the first 518 characters of the `html` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Display meta information on the origin of the HTTP request, e.g. date. Note that it is possible to specify the `user-agent` that the server receives and provides the response (website representation) such that it optimised, e.g. Desktop vs. mobile. If it's not specified, the request will be sent using default values (potentially) containing information about your operating system, screen resolution, keyboard language, IP address and many more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below saves the HTML object's text attribute in HTML format locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ECB.html', 'w', encoding='utf-8') as f:\n",
    "    \n",
    "    f.write(html.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Install the module `BeautifulSoup` via `pip install beautifulsoup4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Parse the BeautifulSoup object `soup` for all Affiliate Links. Hint: In a HTML document all elements that lead to another domain are indicated by an `a` and follow the structure `<a href=\"...\", ... >text</a>`. Hint: Use `soup`'s method `find_all()` where the input argument is the elements' prefix. What object type is the output? Can you iterate over it? How many elements of an Affiliate Link type are contained in the HTML file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Convert the BeautifulSoup object into a \"plain\" Python list object containing the elements' **text** attributes by iterating over it. Hint: Instantiate an empty `list` object, write a for-loop and `append` each element to the list object. You may also remove any unwanted whitespaces by using the `strip` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pro-Tipp\n",
    "Instead of explicitly writing a for-loop when disentangling specific objects from an aggregate object you can use Python's built-in `map` and `lambda` functions as a one-liner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Identify the element which text attribute's value is equal to `\"Research & Publications\"`. Return the element's position (`index`) within the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Obtain this element's value of the `href` attribute. It should be an URL pointing at the domain where the news at Universität Potsdam are collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Write a function which takes a String-type object (e.g. an URL) as input and returns a readily parse-able `BeautifulSoup` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Navigate further into the section of \"Our researchers\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. Open the `new_seed` URL in your browser and enable the CSS SelectorGadget. Highlight the box containing the first article. The other, similar boxes should be highlighted as well. Copy the identified CSS selector and parse through the `news_soup` object but this time over elements corresponding to the CSS selector you found (use `.select()` instead of `find_all()`). Store the subset of elements in a list. You can achieve all of this in one line of code. How many items does this list contain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15. Split the list's elements into their hyperlinks (`href`) and text attributes' values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cannot index by position, needs to be explicit key OR D.values[positional index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ECB_research_dict.json', 'w', encoding='utf-8') as f:\n",
    "    \n",
    "    json.dump(D, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ECB_research_dict.json', 'r', encoding='utf-8') as f:\n",
    "    \n",
    "    D_read = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D == D_read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directories & navigating with `os`\n",
    "\n",
    "We have seen how to change directory with inline magic \n",
    "```python \n",
    "%cd\n",
    "```\n",
    "\n",
    "A more precise way of creating directories, changing and retrieving references is facilitated by `os`:\n",
    "\n",
    "1. os.getcwd() - get current working directory\n",
    "2. os.chdir(path) - change directory to specified `path`\n",
    "3. os.mkdir(path) - create directory named `path`\n",
    "4. os.listdir(path) - lists all files and folders inside of `path`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try & catch\n",
    "\n",
    "When interacting with web services many things can go wrong, i.e. `TimeOutExceptions`, incomplete downloads, unanticipated behavior client-side etc.\n",
    "\n",
    "In general, in absence of an object your program will crash.\n",
    "\n",
    "A way to leave exceptional cases slack / unspecified and instruct your program to run *as if* no exception occurred is to try & catch:\n",
    "\n",
    "```python\n",
    "try:\n",
    "    \n",
    "    operation_1\n",
    "    \n",
    "except:\n",
    "    \n",
    "    operation_2\n",
    "    \n",
    "    OR\n",
    "    \n",
    "    pass\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning intermezzo: Image recognition\n",
    "```python\n",
    "!pip install --user deepface\n",
    "```\n",
    "Using [`deepface`](https://github.com/serengil/deepface), a lightweight face recognition library based on TensorFlow & Keras and openCV comprising state-of-the-art models such as VGG-Face (University of Oxford), Google FaceNet and Facebook DeepFace, we can predict `gender`, `age` and `facial expression` from an image. Note that all image input needs to have the identical dimension. Without going too much into technical details, this is the broad algorithm:\n",
    "\n",
    "1. **Feature engineering**: Re-arranging pixels into one array with each index position, one value [0, 255] per color from the RGB spectrum + **segmentation**\n",
    "2. **Model training**: \n",
    "    - Labeled data\n",
    "    - (Convolutional) Neural networks seem to perform well, probably due to the versatility of interactions between features and correlatedness of neighbouring pixels actually **reduces** to be estimated parameter space and scales well\n",
    "    - flexible way of scanning the image to allow misalignment\n",
    "    - additional features = convolutional layers, \"edges\" etc.\n",
    "    \n",
    "    1) Kernel: randomly distributed sub-image per iteration\n",
    "    \n",
    "    2) Backpropagation: Tweak weights until Kernel matches its corresponding position within the original image (Dot product)\n",
    "    \n",
    "    3) Convolution: \"Scanning of image\" in >= 1 step sizes\n",
    "    \n",
    "    ![Alt Text](2D_Convolution_Animation.gif)\n",
    "    \n",
    "    4) Feature map + bias: summary of neighbouring pixels / features\n",
    "    \n",
    "    5) Rectified Linear Unit (ReLU): Set all negative values in feature map to 0\n",
    "    \n",
    "    $f(x) = max(0, x)$ or $f(x) = ln(1+e^{x})$\n",
    "    \n",
    "    6) Additional (smaller) filter on rectified feature map: $f(x) = max(0, x)$ or mean\n",
    "    \n",
    "    7) Max-pooled (mean-pooled): summary of first filter where highest similarity was found from feature map\n",
    "    \n",
    "    8) Input nodes: flattened max-pool + associated weights (Dot product + bias)\n",
    "    \n",
    "    9) ReLU + Output nodes (as many as classes), e.g. two for female & male\n",
    "    \n",
    "    10) Dot product + bias = 1 or 0 \n",
    "      \n",
    "3. **Cross-Validation**: Trial&Error, adjust parameter weights such that cost function is minimized\n",
    "4. **Evaluation**: Final check on unseen Test dataset, usually several different models/specifications\n",
    "\n",
    "We will be using a pre-trained model from Keras/TensorFlow which is commonly stored in HDF5 file format and can contain both data and model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only for ECB laptops\n",
    "import os\n",
    "os.environ[\"HTTP_PROXY\"] = \"http://ap-python-proxy:x2o7rCPYuN1JuV8H@app-gw-2.ecb.de:8080\"\n",
    "os.environ[\"HTTPS_PROXY\"] = \"https://ap-python-proxy:x2o7rCPYuN1JuV8H@app-gw-2.ecb.de:8080\"\n",
    "\n",
    "from deepface import DeepFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "\n",
    "im = Image.open('./Images/sample_image_0.png')\n",
    "plt.imshow(im)\n",
    "ax = plt.gca()\n",
    "rect = Rectangle((obj['region']['x'],obj['region']['y']),obj['region']['w'],obj['region']['h'],linewidth=1,edgecolor='r',facecolor='none')\n",
    "ax.add_patch(rect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Identifying elements, writing to disk and perform operation using `requests`\n",
    "\n",
    "1. Instantiate an empty data container, e.g. a `list` which will be appended with a well-structured dictionary in each iteration.\n",
    "2. Start a `for`-loop to iterate over each element in the dictionary `D`. Use the `len()` function to specify the upper bound of iterations.\n",
    "3. Assign objects for each field:\n",
    "    - `reseacher_lastname` (Hint: Use the .split() function)\n",
    "    - `researcher_firstname` (Hint: Use the .split() function)\n",
    "    - `reseacher_url` (link to the researcher's dedicated page)\n",
    "    - `researcher_image_url` (link to the researcher's image space)\n",
    "4. `request` the `researcher_image_url` and write the `response`'s `content` to your local directory, e.g.\n",
    "    ```python\n",
    "       file = open(\"./Images/researcher_image_{}.png\".format(i), \"wb\")\n",
    "       file.write(...)\n",
    "       file.close()\n",
    "    ```\n",
    "    Hint: Check if the directory `dirName` = 'Images' exists (os.path.exists(dirName)), if not: create it.\n",
    "    Include a pause of one second after each `request`. Hint: use the `time` module and time.sleep(1)\n",
    "5. Call the `DeepFace.analyze()` function and supply your `file_path` of the image. Retrieve predictions for `age`, `gender` and `emotion`. **Note:** Not every `researcher_image_url` has an actual file associated with it! Implement a fallback-mechanism to record this instance for the corresponding cases.\n",
    "6. Go back to step 2) until you have reached the end of your loop.\n",
    "\n",
    "*Bonus: use a `while`-loop instead of a `for`-loop that runs until you receive a `response.status_code != 200`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Code for exercise 2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting rid of SSL verification errors using requests\n",
    "1. Download the `ECBInterceptionRootCA.cer` from this [page](https://confluence.ecb.de/display/~dirienzo/Work+with+GitHub+repos+from+ECB+laptop) to your `J:` drive.\n",
    "2. Run this code:\n",
    "```python\n",
    "import certifi\n",
    "certifi.where()\n",
    "```\n",
    "This will give you the directory of your certificate that will be used for `requests`.\n",
    "3. Navigate into this directory and copy over everything in the `ECBInterceptionRootCA.cer` including the -----BEGIN CERTIFICATE----- and -----END CERTIFICATE----- tags.\n",
    "\n",
    "This will maintain ECBs security standards (e.g. against man-in-the-middle attacks) and not simply ignore the warnings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Pagination\n",
    "\n",
    "Go to the [news page](https://www.uni-potsdam.de/de/nachrichten/) of the University of Potsdam.\n",
    "\n",
    "You have probably realised that the articles presented on the first news page are not the entire collection of the University of Potsdam. Your goal is to retrieve a complete collection of all articles that are available on the university's website and you can easily apply your new knowledge in a repetitive manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16. Figure out how many pages containing articles content there are in total. You can do it manually by e.g. inspecting the URL when you proceed through the collection in your browser or by checking it programmatically by writing a `while` loop that continues until some condition, such as a status returned from your request, is violated. Make sure to include a short pause (1 second) in order not to overcharge the server that in some cases could lead to a temporary ban of your device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for exercise 3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('articles_links.txt', 'w') as output:\n",
    "    \n",
    "    output.writelines(\"%s\\n\" % line for line in articles_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17. Read in the JSON file you stored in step 17 and iterate over each hyperlink. Split the list into 4 evenly sized chunks and iterate over each chunk. In each iteration, obtain the HTML, parse it and identify the elements of the publication date, the contact, the image's hyperlink/reference and the main text body's length. Note that some, or even all, of these elements may not be available. Define an appropriate data type for each field and append it **as a dictionary** in each iteration to a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partitioning\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asynchronous HTTP requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18. Install the libaries `asyncio`, `aiohttp` and `tqdm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp # !pip install aiohttp\n",
    "import bs4\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio # !pip install nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def get(*args, **kwargs):\n",
    "    \n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        \n",
    "        async with session.get(*args, **kwargs) as resp:\n",
    "            return (await resp.text())\n",
    "        \n",
    "def get_fields(page):\n",
    "    \n",
    "    soup = bs4.BeautifulSoup(page, \"html.parser\")\n",
    "    \n",
    "    try:\n",
    "        publication_date = soup.findAll(class_ =['time', 'up-news-single-date'])[0].text.strip()\n",
    "    except:\n",
    "        publication_date = 'No publication date found.'\n",
    "        \n",
    "    try:\n",
    "        author_name = soup.findAll(class_ =['up-news-single-author'])[0].text[5:].strip()\n",
    "    except:\n",
    "        author_name = 'No author found.'\n",
    "    \n",
    "    try:\n",
    "        image_list = list(map(lambda x: x, soup.select('img')))\n",
    "        image_url = ['https://www.uni-potsdam.de' + x['src'] for x in image_list if x['src'][-4:] == \".jpg\"][0]\n",
    "    except:\n",
    "        image_url = \"No image found.\"\n",
    "        \n",
    "    try:\n",
    "        abstract = soup.select('.up-opener-text-with-border')[0].text.strip()\n",
    "        abstract_length = str(len(abstract))\n",
    "    except:\n",
    "        abstract = 'No abstract found.'\n",
    "        abstract_length = 0\n",
    "    \n",
    "    return publication_date, author_name, image_url, abstract, abstract_length\n",
    "\n",
    "\n",
    "async def save_fields(query):\n",
    "    \n",
    "    url = query\n",
    "    \n",
    "    async with sem:\n",
    "        \n",
    "        page = await get(url, proxy = list(proxies.values())[1], compress=True)\n",
    "        \n",
    "        empty_dict = {}\n",
    "    \n",
    "        [publication_date, author_name, image_url, abstract, abstract_length] = get_fields(page)\n",
    "    \n",
    "        empty_dict['URL'] = url\n",
    "        empty_dict['Publication date'] = publication_date\n",
    "        empty_dict['Author Name'] = author_name\n",
    "        empty_dict['Image URL'] = image_url\n",
    "        empty_dict['Abstract'] = abstract\n",
    "        empty_dict['Abstract Length'] = int(abstract_length)\n",
    "    \n",
    "        results_list.append(empty_dict)\n",
    "\n",
    "sem = asyncio.Semaphore(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "results_list = []\n",
    "\n",
    "loop = asyncio.get_event_loop()\n",
    "f = asyncio.wait([save_fields(d) for d in articles_links_r])\n",
    "result = loop.run_until_complete(f)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "19. Find the missing link that appears in `articles_links_r` but not in `results_list` using a list comprehension. Are there any?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20. Install the `pandas` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "21. Convert the `publication_date` into a `pandas` `datetime` object and plot a time series of published articles on a daily basis. Bonus: Aggregate the time series into monthly frequency. In which month-year were most articles published?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "22. Install the library `matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(monthly_count.index, monthly_count.URL)\n",
    "\n",
    "ax.set(xlabel='Date (monthly)', ylabel='Number of articles published',\n",
    "       title='A simple so-so-looking graph')\n",
    "ax.grid()\n",
    "\n",
    "#fig.savefig(\"monthly_publications.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "23. Install the library `plotly`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install plotly\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "24. Install the `chart-studio` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chart_studio\n",
    "import chart_studio.plotly as py\n",
    "import plotly.graph_objs as go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "25. Log in to [Plotly Chart Studio](https://chart-studio.plotly.com/Auth/login/#/) and obtain your `Username` and `API key`. Store them both line-by-line in a .py file, e.g. name it \"plotly_config.py\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only for ECB laptops\n",
    "import os\n",
    "os.environ[\"HTTP_PROXY\"] = \"http://ap-python-proxy:x2o7rCPYuN1JuV8H@app-gw-2.ecb.de:8080\"\n",
    "os.environ[\"HTTPS_PROXY\"] = \"https://ap-python-proxy:x2o7rCPYuN1JuV8H@app-gw-2.ecb.de:8080\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful for reloading files into memory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(data=[\n",
    "    \n",
    "    go.Scatter(name='Published articles', x = list(monthly_count.index),\n",
    "    y = list(monthly_count['URL']))\n",
    "    \n",
    "])\n",
    "\n",
    "fig.layout.update(title = go.layout.Title(\n",
    "                        text='Published articles (monthly)'))\n",
    "\n",
    "fig.layout.update(yaxis= go.layout.YAxis(title=go.layout.yaxis.Title(\n",
    "                        text='Count')))\n",
    "\n",
    "fig.layout.update(xaxis = go.layout.XAxis(title = go.layout.xaxis.Title(text = 'Date'), rangeslider = dict(visible = True)));\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Webscraping Workshop",
   "language": "python",
   "name": "webscraping-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
